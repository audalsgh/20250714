# 16일차

## CNN (Convolutional Neural Network) 정리
컨볼루션에 대한 애니메이션
https://claude.ai/public/artifacts/2cebc728-66b5-414a-9e97-991f60a2a7e1

왜 중요한가?<br>
입력 이미지에 커널을 슬라이딩하며 국소 영역의 정보를 요약(내적)해 새로운 특징 맵을 만듬. <br>전반의 패턴(엣지, 코너, 질감 등)을 ‘위치-불변성’ 있게 학습할 수 있어, 복잡한 패턴도 단계별로 계층화해 인식 가능<br>
스트라이드와 패딩 설정으로 출력 맵의 공간 해상도(크기)를 자유롭게 설정가능.<br>
완전연결 층에선 모든 입력 뉴런과 출력 뉴런이 연결되있어 ‘전역적’ 특징을 학습하지만, 파라미터 수가 많아 과적합 위험도 존재.

**요약 : CNN 학습 흐름<br>
입력 -> (Convolution -> Kernel) = 계층별로 국소 특징 추출 ->  CNN에서 합성곱 층을 거친 뒤에 나오는 2D 출력인 Feature Map 생성 -> Pooling으로 해상도 축소<br>
Flatten으로 1차원 벡터화 -> Fully Connected Layer이 만들어짐-> Activation Function 활성화 함수 적용해 근사 -> Loss 계산이 가능해짐 -> Optimizer로 파라미터 업데이트 -> Epoch 반복 + Regularization 적용**

| 용어                              | 설명                                                                                                              |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Convolution (합성곱)**          | 입력 이미지에 커널(필터)을 슬라이딩하면서 내적 연산을 수행해 특징 맵(feature map)을 생성                           |
| **Kernel = Filter (커널 = 필터)** | 3×3, 5×5 같은 작은 크기의 가중치 행렬로, 각 국소 영역과 곱해져 입력에서 특정 패턴(엣지, 텍스처 등)을 추출            |
| **Stride (스트라이드)**           | 커널을 이동시킬 때 한 번에 이동하는 픽셀 수. (Stride=1: 한 픽셀씩, Stride=2: 두 픽셀씩 → 출력 맵 크기 변화)       |
| **Padding (패딩)**               | 입력 주변에 0(또는 다른 값)을 추가해 출력 크기를 조절하거나 경계 정보 손실을 방지 (‘same’: 입력과 같게, ‘valid’: 순수 합성곱) |
| **Activation Function**          | 합성곱·완전연결 층의 선형 출력에 비선형성을 부여. (대표: **ReLU**, Sigmoid, Tanh 등)                                |
| **Pooling (풀링)**               | 특징 맵 크기 축소 및 위치 변동 강인성 부여<br>- **Max Pooling**: 영역 내 최대값<br>- **Average Pooling**: 영역 내 평균값 |
| **Flatten**                      | 다차원 feature map을 1차원 벡터로 변환                                                                            |
| **Fully Connected Layer**        | 평탄화된 벡터를 입력으로 받아 최종 클래스 점수나 회귀값을 예측                                                     |
| **Epoch (에폭)**                 | 전체 학습 데이터를 한 번 모두 사용해 파라미터를 업데이트한 횟수                                                      |
| **Batch (배치)**                 | 한 번에 신경망에 입력으로 넣어 학습시키는 샘플 묶음. 배치 크기에 따라 학습 안정성과 속도 변화                         |
| **Loss Function**                | 모델 예측값과 실제값 간 차이를 수치화<br>– 분류: Cross-Entropy<br>– 회귀: MSE                                        |
| **Optimizer**                    | 손실 함수를 최소화하도록 파라미터를 업데이트하는 알고리즘<br>ex) SGD, Momentum, RMSprop, Adam 등                     |
| **Regularization (정규화)**       | 모델 복잡도 제어로 과적합 방지<br>– L1/L2 페널티: 가중치 크기 제어<br>– Dropout: 학습 시 뉴런 일부 무작위 비활성화    |
